{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc0242cf-2544-495c-a204-d9a3b540e29e",
   "metadata": {},
   "source": [
    "# I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0ec0dc-ac79-456c-9ab4-2508a2956dc6",
   "metadata": {},
   "source": [
    "## 1. Domain-specific area (rewrite)\n",
    "In this work, we present a text classifier for detecting fake news. Fake news, also known as misinformation, refers to false or misleading information presented as if it were real news. It has become a major issue in recent years, with the proliferation of social media platforms and the ease with which false information can be disseminated. The negative impact of fake news cannot be understated, as it can lead to harm to individuals and society as a whole.\n",
    "\n",
    "To address this problem, we have developed a machine learning-based text classifier that can accurately identify fake news articles. The classifier is trained on a large dataset of real and fake news articles, and uses various features of the text, such as word counts and sentiment, to make predictions.\n",
    "\n",
    "We evaluate the performance of the classifier using several metrics, and show that it is able to achieve high accuracy in detecting fake news. We also discuss the potential applications of the classifier, including its use by news organizations to fact-check articles, and by social media platforms to combat the spread of fake news."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76187fc-0793-450d-970f-b54dc129f3b3",
   "metadata": {},
   "source": [
    "## 2. Objectives (add references)\n",
    "This project aims to find a suitable way to perform text classification in news articles in order to classify if the article is or is not fake news. In order to adapt ourselves to the social media era and avoid the spread of misinformation we need to improve the ways we validate what is truth and what is not. Historically, we've seen that fake news can contribute to problems such as:\n",
    "1. Damaging the reputation of people through spreading misinformaiton. [reference]\n",
    "2. Advertise false propaganda in order to misguide elections and/or election results. [reference]\n",
    "3. Generate confirmation bias manipulating one's perception of reality. [reference]\n",
    "4. Estimulating conflicts in a situation where polarity is arising in society. [reference]\n",
    "\n",
    "We've also seen the widespread of fake news during COVID which, according to studies [reference], have been one of the causes of vaccine hesitancy, which has lead to unnecessary deaths all over the world.\n",
    "\n",
    "This work consists in an automated way to fact-check news in order to tackle the problems above and many more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c6e39f-9a77-44c6-97e2-5fee1e4e8982",
   "metadata": {},
   "source": [
    "## 3. Dataset\n",
    "\n",
    "### 3.1. Description\n",
    "In this work we will explore a dataset consisting of two CSV files containing classified fake and real news and we will use it to train our Machine Learning Model in order to be able to evaluate and classify other news. The language is english and the dataset consists of the following features:\n",
    "1. title: The title of the news article.\n",
    "2. text: The article itself.\n",
    "3. subject: Examples of a subject could be: politics, middle-east and news.\n",
    "4. date: The date that the article was published.\n",
    "### 3.2. Dataset size\n",
    "The first CSV file called 'True.csv' holding the articles categorized as not fake news consists of 21417 articles. The second one called 'Fake.csv' consists of 23481 articles.\n",
    "### 3.3. Data types\n",
    "All the data types are strings, except for the last column in the dataset which is a Date.\n",
    "### 3.4. Source\n",
    "Source: 'Fake and real news - Classifying the news' taken from kaggle. [link]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba24599-6576-4b84-9d56-bd60b1ce0ac8",
   "metadata": {},
   "source": [
    "## 4. Evaluation methodology\n",
    "\n",
    "For the evaluation of the model the technique being used here is accuracy, since it is a simple and quick way to give a perspective of the performance in one single number and also very easy to use with the classification algorithm we are using (logistic regression). We are using numpy to calculate that based on the results of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe30b9f6-e173-4f5a-8d24-eada7dda741b",
   "metadata": {},
   "source": [
    "# II. Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f123f17-216c-44ce-8d0a-63d4fdd988d5",
   "metadata": {},
   "source": [
    "## 5. Preprocessing\n",
    "\n",
    "### 5.1. Text representation\n",
    "As for the text representation and lexical analysis we are using a Word2Vec model with the gensim library. The reason we decided to use this is due to the fact that it keeps information about the ordering of the words in the vector, which is going to be useful to later analyze the bigrams (words that keep appearing together) which can be informative in order to understand properties of fake news articles.\n",
    "### 5.2 Pre-processing the data\n",
    "As for the preprocessing and text normalization step, we are using the following techniques:\n",
    "1. Tokenizing\n",
    "Using nltk to separate each sentence into tokens.\n",
    "2. Removing stopwords\n",
    "We are also using nltk's stopwords list for the english vocabulary in order to remove words that have no meaning (such as 'is' and 'are').\n",
    "\n",
    "### 5.3 File type format\n",
    "As per the file type format, the raw data is in two CSV files, which will then be added labels and merged in order to extract the features for the classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36564ecd-8746-4ca7-bf52-063fc2cc3cb4",
   "metadata": {},
   "source": [
    "## Loading and inspect the dataset: https://www.kaggle.com/code/arund8888/titanic-classification-models-score-73\n",
    "TO DO:\n",
    "1. Create dataframes from CSVs (DONE)\n",
    "2. Add column label to both dataframes (DONE)\n",
    "3. Merge both the dataframes into one (DONE)\n",
    "4. Describe columns/find and fix missing data (is it worth it?)\n",
    "5. Plot two graphics: number of articles per subject and number of fake news per month/year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb0b88bc-2822-4442-9500-5201cc518047",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pandas to load the dataset\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "fake_df = pd.read_csv('Fake.csv')\n",
    "true_df = pd.read_csv('True.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eddd8111-40da-4093-a082-6e8109cac896",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>subject</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>20650</th>\n",
       "      <td>MALIA OBAMA TO ATTEND UNIVERSITY With 5.9% Acc...</td>\n",
       "      <td>While both of her parents travel around the co...</td>\n",
       "      <td>left-news</td>\n",
       "      <td>May 1, 2016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1200</th>\n",
       "      <td>Trump Just Offered To Testify Under Oath And ...</td>\n",
       "      <td>After James Comey called him a liar five times...</td>\n",
       "      <td>News</td>\n",
       "      <td>June 9, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10743</th>\n",
       "      <td>WOW! SEAN SPICER Destroys BBC, New York Times ...</td>\n",
       "      <td>Here is the Gateway Pundit s  real news  accou...</td>\n",
       "      <td>politics</td>\n",
       "      <td>May 30, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16427</th>\n",
       "      <td>Catalan leader's address canceled: regional go...</td>\n",
       "      <td>MADRID (Reuters) - A statement by the leader o...</td>\n",
       "      <td>worldnews</td>\n",
       "      <td>October 26, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>Arkansas Republicans Pass Bill Giving Rapists...</td>\n",
       "      <td>A vicious new law in Arkansas proves that Repu...</td>\n",
       "      <td>News</td>\n",
       "      <td>February 3, 2017</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>Factbox: Trump on Twitter (December 12) - Demo...</td>\n",
       "      <td>The following statementsÂ were posted to the ve...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>December 13, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3990</th>\n",
       "      <td>Trump could target 'carried interest' tax loop...</td>\n",
       "      <td>WASHINGTON (Reuters) - The Trump administratio...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>April 30, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14271</th>\n",
       "      <td>WATCH VETERAN Embarrass Trump Hater In Kansas ...</td>\n",
       "      <td>This veteran exposes Trump hating protester in...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Mar 17, 2016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12496</th>\n",
       "      <td>HYSTERICAL! TRUMP LIFE: âThere ainât a brother...</td>\n",
       "      <td>#SourcesHaveConfirmed that Trump will lock her...</td>\n",
       "      <td>politics</td>\n",
       "      <td>Nov 5, 2016</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>440</th>\n",
       "      <td>Congressman Conyers has not thought about resi...</td>\n",
       "      <td>(Reuters) - Democratic Representative John Con...</td>\n",
       "      <td>politicsNews</td>\n",
       "      <td>November 30, 2017</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows Ã 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   title  \\\n",
       "20650  MALIA OBAMA TO ATTEND UNIVERSITY With 5.9% Acc...   \n",
       "1200    Trump Just Offered To Testify Under Oath And ...   \n",
       "10743  WOW! SEAN SPICER Destroys BBC, New York Times ...   \n",
       "16427  Catalan leader's address canceled: regional go...   \n",
       "2704    Arkansas Republicans Pass Bill Giving Rapists...   \n",
       "...                                                  ...   \n",
       "192    Factbox: Trump on Twitter (December 12) - Demo...   \n",
       "3990   Trump could target 'carried interest' tax loop...   \n",
       "14271  WATCH VETERAN Embarrass Trump Hater In Kansas ...   \n",
       "12496  HYSTERICAL! TRUMP LIFE: âThere ainât a brother...   \n",
       "440    Congressman Conyers has not thought about resi...   \n",
       "\n",
       "                                                    text       subject  \\\n",
       "20650  While both of her parents travel around the co...     left-news   \n",
       "1200   After James Comey called him a liar five times...          News   \n",
       "10743  Here is the Gateway Pundit s  real news  accou...      politics   \n",
       "16427  MADRID (Reuters) - A statement by the leader o...     worldnews   \n",
       "2704   A vicious new law in Arkansas proves that Repu...          News   \n",
       "...                                                  ...           ...   \n",
       "192    The following statementsÂ were posted to the ve...  politicsNews   \n",
       "3990   WASHINGTON (Reuters) - The Trump administratio...  politicsNews   \n",
       "14271  This veteran exposes Trump hating protester in...      politics   \n",
       "12496  #SourcesHaveConfirmed that Trump will lock her...      politics   \n",
       "440    (Reuters) - Democratic Representative John Con...  politicsNews   \n",
       "\n",
       "                     date  label  \n",
       "20650         May 1, 2016  False  \n",
       "1200         June 9, 2017  False  \n",
       "10743        May 30, 2017  False  \n",
       "16427   October 26, 2017    True  \n",
       "2704     February 3, 2017  False  \n",
       "...                   ...    ...  \n",
       "192    December 13, 2017    True  \n",
       "3990      April 30, 2017    True  \n",
       "14271        Mar 17, 2016  False  \n",
       "12496         Nov 5, 2016  False  \n",
       "440    November 30, 2017    True  \n",
       "\n",
       "[20000 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding a label to the true and fake dataframes so we can use the classifier next\n",
    "\n",
    "fake_df['label'] = 'False'\n",
    "true_df['label'] = 'True'\n",
    "\n",
    "# Merging the two dataframes into one (we are going to need this in order to train the model)\n",
    "data = pd.concat([fake_df, true_df])\n",
    "data\n",
    "\n",
    "# Shuffling the information\n",
    "data = data.sample(frac = 1)\n",
    "\n",
    "# Since there are too much rows (44898) in this dataframe and it is too costly to do operations such as iterate through it, I am going to use a subset of it\n",
    "data_copy = data.head(20000) # This is what we are going to be using from now on\n",
    "data_copy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ae9cb9-f519-4c4f-ad1d-c85594cafb5a",
   "metadata": {},
   "source": [
    "### Exploratory data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95dfc9d5-a6c1-4998-8728-2c5fe14d6122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subjects:  ['News' 'left-news' 'politicsNews' 'politics' 'worldnews'\n",
      " 'Government News' 'US_News' 'Middle-east']\n"
     ]
    }
   ],
   "source": [
    "# Printing the unique subjects in each dataset\n",
    "print(\"Subjects: \", data_copy.subject.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d7a72ee-c286-4315-a51e-7211678e47fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns: Index(['title', 'text', 'subject', 'date', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Printing the different columns\n",
    "print(\"Columns:\", data_copy.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89d051ab-219d-44d8-81d3-75d153491860",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the number of fake news per subject\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sb\n",
    "\n",
    "# data['subject']\n",
    "# plt.figure(figsize=(10,8))\n",
    "# sb.countplot(data[\"subject\"])\n",
    "# plt.title(\"News\")\n",
    "# plt.xlabel(\"Politics\")\n",
    "# plt.xlabel(\"Government News\")\n",
    "# plt.xlabel(\"left-news\")\n",
    "# plt.xlabel(\"US_News\")\n",
    "# plt.xlabel(\"Middle-east\")\n",
    "# plt.ylabel(\"politicsNews\")\n",
    "# plt.ylabel(\"worldnews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bc5593-4fbf-4203-b420-55ed7db4d72f",
   "metadata": {},
   "source": [
    "## Preprocessing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2bdfc1b6-9722-45d7-a65c-dd5008846fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DELETE\n",
    "# Generating a list of sentences so that we can preprocess and analyze later\n",
    "fake_articles = fake_df['title']\n",
    "true_articles = true_df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0009c1a-fcb6-43d8-8168-0cc62ca934c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Preprocess the sentences\n",
    "def preprocess(sentences):\n",
    "    # Tokenize the sentences\n",
    "    sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
    "    # Lowercase the words\n",
    "    sentences = [[word.lower() for word in sentence] for sentence in sentences]\n",
    "    # Removing stopwords\n",
    "    stop_words = stopwords.words('english')\n",
    "    sentences = [word for word in sentences if not word in stop_words]\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9f9ed7-7bfe-4dda-94b6-289f78db6acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we are creating a new column in the dataframe called 'clean_text' and adding the results of the preprocess method\n",
    "\n",
    "sentences = data_copy['text']\n",
    "sentences = preprocess(sentences)\n",
    "\n",
    "data_copy['clean_text'] = sentences;\n",
    "data_copy.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b0d60b-147e-4eaf-9968-8f5f9b393da6",
   "metadata": {},
   "source": [
    "## Lexical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747bd082-540b-4153-a7aa-ac684f2a192a",
   "metadata": {},
   "source": [
    "## Text representation: Word2Vec (Re-do: https://www.kaggle.com/code/hamishdickson/training-and-plotting-word2vec-with-bigrams)\n",
    "TO DO:\n",
    "1. Generate the model according to the link\n",
    "2. Find a good way to plot the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b9ad4-a453-4d2e-b4cb-d4abe7b4ae9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Create a Word2Vec model and train it on the preprocessed sentences\n",
    "fake_lexical_model = Word2Vec(preprocessed_fake_sentences, window=5, min_count=1, workers=4)\n",
    "fake_lexical_model.train(preprocessed_fake_sentences, total_examples=len(sentences), epochs=10)\n",
    "\n",
    "true_lexical_model = Word2Vec(preprocessed_true_sentences, window=5, min_count=1, workers=4)\n",
    "true_lexical_model.train(preprocessed_true_sentences, total_examples=len(sentences), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114b3ffb-0e1c-443e-84fa-45591a685ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the fake lexicalmodel\n",
    "\n",
    "# Length of the model:\n",
    "print(\"Model length: \", len(fake_lexical_model.wv.key_to_index))\n",
    "\n",
    "# how many dimensions?\n",
    "print(\"Model dimensions:\", len(fake_lexical_model.wv['embarrassing']))\n",
    "\n",
    "# Finding similar terms\n",
    "fake_lexical_model.wv.most_similar('corruption', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c7dcac4-e3f6-4501-83f3-26c3ec96b6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the true lexical model\n",
    "\n",
    "# Length of the model:\n",
    "print(\"Model length: \", len(true_lexical_model.wv.key_to_index))\n",
    "\n",
    "# how many dimensions?\n",
    "print(\"Model dimensions:\", len(true_lexical_model.wv['rating']))\n",
    "\n",
    "# Finding similar terms\n",
    "true_lexical_model.wv.most_similar('rating', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b92487-fb32-42a4-b5f0-4d7e0e418394",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim.models.word2vec import Text8Corpus\n",
    "from gensim.models.phrases import Phrases, ENGLISH_CONNECTOR_WORDS\n",
    "# Create training corpus. Must be a sequence of sentences (e.g. an iterable or a generator).\n",
    "sentences = preprocessed_fake_sentences[:10]\n",
    "\n",
    "# Train a toy phrase model on our training corpus.\n",
    "phrase_model = Phrases(sentences, min_count=1, threshold=1, connector_words=ENGLISH_CONNECTOR_WORDS)\n",
    "\n",
    "# Apply the trained phrases model to a new, unseen sentence.\n",
    "new_sentence = preprocessed_fake_sentences[11:20]\n",
    "phrase_model[new_sentence]\n",
    "\n",
    "# The toy model considered \"trees graph\" a single phrase => joined the two\n",
    "# tokens into a single \"phrase\" token, using our selected `_` delimiter.\n",
    "# Apply the trained model to each sentence of a corpus, using the same [] syntax:\n",
    "for sent in phrase_model[sentences]:\n",
    "    pass\n",
    "\n",
    "# Update the model with two new sentences on the fly.\n",
    "# phrase_model.add_vocab([[\"hello\", \"world\"], [\"meow\"]])\n",
    "\n",
    "# Export the trained model = use less RAM, faster processing. Model updates no longer possible.\n",
    "frozen_model = phrase_model.freeze()\n",
    "\n",
    "# Apply the frozen model; same results as before:\n",
    "frozen_model[new_sentence]\n",
    "\n",
    "\n",
    "# # Save / load models.\n",
    "frozen_model.save(\"/tmp/my_phrase_model.pkl\")\n",
    "model_reloaded = Phrases.load(\"/tmp/my_phrase_model.pkl\")\n",
    "model_reloaded[preprocessed_fake_sentences[21:22]]  # apply the reloaded model to a sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a44d20d-a4fb-49c4-b15d-d9300d2336d8",
   "metadata": {},
   "source": [
    "## 7. Classification approach\n",
    "\n",
    "### 7.1 Features and Labels\n",
    "For the classifier we are using two features: the 'title' which in the dataframe is the representation of the article and the 'label' which is the feature that tells which articles are true or fake.\n",
    "\n",
    "### 7.2 Classifier\n",
    "For the classifier we are using the logistic regression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ed837-7e3a-4782-88b8-7b2df821618c",
   "metadata": {},
   "source": [
    "## Creating the model for the classifier\n",
    "TO DO:\n",
    "1. Make it work\n",
    "2. Add column 'prediction' to dataframe and add prediction information for each sentence\n",
    "3. Plot the number of fake news per subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1767a2e2-3080-40c7-82cd-7e126932dc1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fake news classifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Pre-process the data\n",
    "texts = data['text'].values\n",
    "labels = data['label'].values\n",
    "\n",
    "# Tokenize the texts\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "# Split the data into a training set and a test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, labels, test_size=0.33, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Test the model\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "for x in range(10):\n",
    "    print(texts[x] + ' | ' + y_pred[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b21a2f-ee54-4780-ad01-147aff4a37c7",
   "metadata": {},
   "source": [
    "## 6. Baseline performance\n",
    "Describe and justify the baseline against which you are going to compare the performance\n",
    "of your chosen approach. This can be an already published baseline (e.g. cited in the\n",
    "literature) or the results of a basic algorithm that you implement yourself. The baseline\n",
    "should represent a meaningful benchmark for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443c32f4-2a96-47d7-b74e-1aea1039ed2b",
   "metadata": {},
   "source": [
    "## 8. Coding style\n",
    "Your code is expected to meet certain standards as described by accepted coding\n",
    "conventions. This includes code indentation, avoiding unnamed numerical constants and\n",
    "undue use of string literals, assigning meaningful names to variables and subroutines, etc.\n",
    "The code is expected to be fully commented, including variables, sub-routines and calls to\n",
    "library methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5bfb8-0862-4d87-810a-8af55f89dba8",
   "metadata": {},
   "source": [
    "# III. Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d3f331-e574-464f-83fb-e3b94414f74b",
   "metadata": {},
   "source": [
    "## 9. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5dd550-ea0f-4289-84fb-a147dd903291",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = np.mean(y_pred == y_test)\n",
    "print(f'Accuracy: {accuracy:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea0806-fb4b-4990-9c57-36f5bcf29f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the model above on new data\n",
    "\n",
    "# Pre-process the new data\n",
    "X_new = vectorizer.transform(true_sentences[10])\n",
    "\n",
    "# Use the model to make predictions on the new data\n",
    "y_pred = model.predict(X_new)\n",
    "\n",
    "# Print the predictions\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f966415-36fe-4211-aafe-3ce125ac0304",
   "metadata": {},
   "source": [
    "## 10. Summary and conclusions\n",
    "Provide a reflective evaluation of the project in light of your results. Describe its\n",
    "contributions to the problem area, and discuss the extent to which your solution is\n",
    "transferable to other domain-specific areas. Discuss the extent to which your approach can\n",
    "be replicated by others, e.g. using different programming languages, development\n",
    "environments, libraries and algorithms. Review the potential benefits and drawbacks of\n",
    "alternative approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f1166-abec-4be4-a05c-c95462efa721",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836f9de8-a253-4a5d-9baa-8e6cd10453ab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
